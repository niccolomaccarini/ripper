{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bb2ec35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tpe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4b48073654a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-4b48073654a2>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mPlot1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearningCurvePlot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mproblem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' problem'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgammas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             learning_curve = average_over_repetitions(smoothing_window, plot, n_repetitions, method,\n\u001b[0m\u001b[0;32m     54\u001b[0m                                                           n_episodes, gamma, problem, rwarmup)\n\u001b[0;32m     55\u001b[0m             \u001b[0mPlot1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_curve\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'test gamma: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-4b48073654a2>\u001b[0m in \u001b[0;36maverage_over_repetitions\u001b[1;34m(smoothing_window, plot, n_repetitions, method, n_episodes, gamma, problem, rwarmup)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tpe'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtpe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tpe' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from Helper import LearningCurvePlot, smooth\n",
    "\n",
    "def average_over_repetitions(smoothing_window, plot, n_repetitions, method, n_episodes, gamma, problem, rwarmup):\n",
    "    losses_results = np.empty([n_repetitions,n_episodes]) # Losses array\n",
    "    now = time.time()\n",
    "    \n",
    "    if method == 'tpe':\n",
    "        G = tpe\n",
    "    else:\n",
    "        G = random_search\n",
    "    \n",
    "    for rep in range(n_repetitions): # Loop over repetitions\n",
    "        losses, configs = G(problem = problem, function_evaluations=n_episodes, random_warmup=rwarmup, gamma=gamma)\n",
    "        losses_results[rep] = losses\n",
    "    \n",
    "    print('Running one setting takes {} minutes'.format((time.time()-now)/60))\n",
    "    learning_curve = np.mean(losses_results,axis=0) # average over repetitions\n",
    "    print('Employing the ' + method + ' method on the ' + problem + ' problem, we obtain an average best loss of: ' + str(min(learning_curve)))\n",
    "    learning_curve = smooth(learning_curve,smoothing_window) # additional smoothing\n",
    "    return learning_curve\n",
    "\n",
    "def experiment():\n",
    "    ####### Settings\n",
    "    # Experiment    \n",
    "    n_repetitions = 20\n",
    "    smoothing_window = 24\n",
    "    n_episodes = 150\n",
    "    gammas = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    problems = ['interactive']\n",
    "    methods = ['tpe','rse']\n",
    "    rwarmup = 10\n",
    "    gamma = 0.1\n",
    "    \n",
    "    # Plotting parameters\n",
    "    plot = True\n",
    "    \n",
    "    # Nice labels for plotting\n",
    "    labels = {'interactive': 'Interactive', 'good_range': 'Good range', 'bad_range': 'Bad range', 'rs': 'random search',\n",
    "             'tpe': 'tpe'}\n",
    "\n",
    "    \n",
    "    ####### Experiments\n",
    "        \n",
    "    #problem = 'interactive'\n",
    "    method = 'tpe'\n",
    "    #Plot1 = LearningCurvePlot(title = labels[problem] + ' problem solved with ' + labels[method])\n",
    "    \n",
    "    for problem in problems:\n",
    "        Plot1 = LearningCurvePlot(title = labels[problem] + ' problem')\n",
    "        for gamma in gammas:\n",
    "            learning_curve = average_over_repetitions(smoothing_window, plot, n_repetitions, method,\n",
    "                                                          n_episodes, gamma, problem, rwarmup)\n",
    "            Plot1.add_curve(learning_curve,label = 'test gamma: ' + str(gamma))\n",
    "        Plot1.save('test_' + labels[method] + '_' + labels[problem] + '.png')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    experiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88daecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(problem, function_evaluations=150, **kwargs):\n",
    "    \"\"\"\n",
    "    Function that performs random search on the given problem. It uses the \n",
    "    ranges and sampling types defined in GET_RANGES(problem) (see utils.py).\n",
    "\n",
    "    Arguments:\n",
    "      - problem (str): the prolbem identifier\n",
    "      - function_evaluations (int): the number of configurations to evaluate\n",
    "      - **kwargs: any other keyword arguments \n",
    "\n",
    "    Returns:\n",
    "      - history (list): A list of the observed losses\n",
    "      - configs (list): A list of the tried configurations. Every configuration is a dictionary\n",
    "                        mapping hyperparameter names to the chosen values \n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    configs = []\n",
    "\n",
    "    # get all information about the hyperparameters we want to tune for this problem\n",
    "    # (see utils.py) for the form of this.  \n",
    "    RANGES = GET_RANGES(problem) \n",
    "\n",
    "    # example of a hyperparameter configuration. you of course want to change the names and values\n",
    "    # according to the problem and the values you sample\n",
    "    config = {}\n",
    "\n",
    "    for hyperparameter in RANGES:\n",
    "        set_range = RANGES[hyperparameter]['range']\n",
    "        set_type = RANGES[hyperparameter]['type']\n",
    "        set_sample = RANGES[hyperparameter]['sample']\n",
    "        \n",
    "        if set_type == 0:\n",
    "            config[hyperparameter] = np.random.choice(set_range)\n",
    "        if set_type == 1:\n",
    "            if set_sample == 0:\n",
    "                config[hyperparameter] = np.random.uniform(RANGES[hyperparameter]['range'][0], RANGES[hyperparameter]['range'][1])\n",
    "            if set_sample == 1:\n",
    "                config[hyperparameter] = np.exp(np.random.uniform(np.log(RANGES[hyperparameter]['range'][0]), np.log(RANGES[hyperparameter]['range'][1])))\n",
    "        if set_type == 2:\n",
    "                if set_sample == 0:\n",
    "                    config[hyperparameter] = round(np.random.uniform(RANGES[hyperparameter]['range'][0], RANGES[hyperparameter]['range'][1]))\n",
    "                if set_sample == 1:\n",
    "                    config[hyperparameter] = round(np.exp(np.random.uniform(np.log(RANGES[hyperparameter]['range'][0]), np.log(RANGES[hyperparameter]['range'][1]))))\n",
    "            \n",
    "    configs += [config]\n",
    "    \n",
    "    # once you have a configuration 'config' in the form of a dictionary mapping from \n",
    "    # hyperparameter -> value you can evaluate it using this function\n",
    "    loss = GET_CONFIG_PERFORMANCE(config, problem)\n",
    "    history += [loss]\n",
    "\n",
    "    # TODO: implement the rest of the function\n",
    "    for step in range(function_evaluations - 1):\n",
    "        config = {}\n",
    "\n",
    "        for hyperparameter in RANGES:\n",
    "            set_range = RANGES[hyperparameter]['range']\n",
    "            set_type = RANGES[hyperparameter]['type']\n",
    "            set_sample = RANGES[hyperparameter]['sample']\n",
    "            \n",
    "            if set_type == 0:\n",
    "                config[hyperparameter] = np.random.choice(set_range)\n",
    "            if set_type == 1:\n",
    "                if set_sample == 0:\n",
    "                    config[hyperparameter] = np.random.uniform(RANGES[hyperparameter]['range'][0], RANGES[hyperparameter]['range'][1])\n",
    "                if set_sample == 1:\n",
    "                    config[hyperparameter] = np.exp(np.random.uniform(np.log(RANGES[hyperparameter]['range'][0]), np.log(RANGES[hyperparameter]['range'][1])))\n",
    "            if set_type == 2:\n",
    "                if set_sample == 0:\n",
    "                    config[hyperparameter] = round(np.random.uniform(RANGES[hyperparameter]['range'][0], RANGES[hyperparameter]['range'][1]))\n",
    "                if set_sample == 1:\n",
    "                    config[hyperparameter] = round(np.exp(np.random.uniform(np.log(RANGES[hyperparameter]['range'][0]), np.log(RANGES[hyperparameter]['range'][1]))))\n",
    "                \n",
    "        configs += [config]\n",
    "        \n",
    "        #report loss\n",
    "        loss = GET_CONFIG_PERFORMANCE(config, problem)\n",
    "        history += [loss]\n",
    "\n",
    "    \n",
    "    return history, configs\n",
    "\n",
    "\n",
    "def tpe(problem, function_evaluations=150, random_warmup=30, gamma=0.2, **kwargs):\n",
    "    \"\"\"\n",
    "    Function that uses Tree Parzen Estimator (TPE) to tune the hyperparameters of the \n",
    "    given problem. It uses the ranges and sampling types defined in GET_RANGES(problem) \n",
    "    (see utils.py).\n",
    "\n",
    "    Arguments:\n",
    "      - problem (str): the problem identifier\n",
    "      - function_evaluations (int): the number of configurations to evaluate\n",
    "      - random_warmup (int): the number of initial iterations during which we perform random \n",
    "                             search\n",
    "      - gamma: the value of gamma that determines the cutting point [good partition, bad partition]\n",
    "      - **kwargs: any other keyword arguments \n",
    "\n",
    "    Returns:\n",
    "      - history (list): A list of the observed losses\n",
    "      - configs (list): A list of the tried configurations. Every configuration is a dictionary\n",
    "                        mapping hyperparameter names to the chosen values \n",
    "    \"\"\"\n",
    "\n",
    "    history = []\n",
    "    configs = []\n",
    "    eps = 1e-250\n",
    "\n",
    "    # define a function to sum different pdf of truncated normal distributions with same shape and same scale\n",
    "    def sum_pdf(x, means, myclip_a, myclip_b):\n",
    "        tot_sum = 0\n",
    "        ordered_means = sorted(means)\n",
    "        for i in range(len(ordered_means)):\n",
    "            m = ordered_means[i]\n",
    "            # as in the original paper, the scale is defined as the maximum distance between the left and right neighbours,\n",
    "            # but clipped to remain less than 1 and bigger than 0.01\n",
    "            scale = min(max(m - ordered_means[max(i-1,0)], ordered_means[min(i+1, len(means)-1)] - m, 0.01), 0.5) + eps\n",
    "            a, b = (myclip_a - m) / scale, (myclip_b - m) / scale\n",
    "            tot_sum += truncnorm.pdf(x, a, b, loc = m, scale = scale)/len(means)\n",
    "        return tot_sum\n",
    "\n",
    "    # define a function to take samples from a mixture of truncated normal distributions with same shape ans same scale\n",
    "    def sample_from_mixture(means, myclip_a, myclip_b, size):\n",
    "        samples = []\n",
    "        ordered_means = sorted(means)\n",
    "        for j in range(size):\n",
    "            i = np.random.choice(range(len(means)))\n",
    "            m = ordered_means[i]\n",
    "            scale = min(max(m - ordered_means[max(i-1,0)], ordered_means[min(i+1, len(means)-1)] - m), 1) + eps\n",
    "            a, b = (myclip_a - m) / scale, (myclip_b - m) / scale\n",
    "            chosen_distribution = truncnorm(a, b, loc = m, scale = scale)\n",
    "            sampled_value = chosen_distribution.rvs()\n",
    "            samples += [sampled_value]\n",
    "        return samples\n",
    "                                               \n",
    "    # get all information about the hyperparameters we want to tune for this problem\n",
    "    # (see utils.py) for the form of this.  \n",
    "    RANGES = GET_RANGES(problem) \n",
    "\n",
    "    # example of a hyperparameter configuration. you of course want to change the names and values\n",
    "    # according to the problem and the values you sample\n",
    "    history_warmup, configs_warmup = random_search(problem, random_warmup)\n",
    "    history += history_warmup\n",
    "    configs += configs_warmup\n",
    "\n",
    "    for step in range(function_evaluations - random_warmup):\n",
    "        config = {}\n",
    "\n",
    "        for hyperparameter in RANGES:\n",
    "            values = [c[hyperparameter] for c in configs if hyperparameter in c]\n",
    "            losses = [history[i] for i in range(len(configs)) if hyperparameter in configs[i]]\n",
    "            ordering_permutation = np.argsort(losses)\n",
    "            ordered_values = [values[i] for i in ordering_permutation]\n",
    "            good_values = values[0: round(len(values)*gamma)]\n",
    "            bad_values = values[round(len(values)*gamma):]\n",
    "            set_range = RANGES[hyperparameter]['range']\n",
    "            set_type = RANGES[hyperparameter]['type']\n",
    "            set_sample = RANGES[hyperparameter]['sample']\n",
    "            myclip_a = RANGES[hyperparameter]['range'][0]\n",
    "            myclip_b = RANGES[hyperparameter]['range'][1]\n",
    "            \n",
    "            # the following line checks whether conditional hyperparameters should be considered\n",
    "            if (problem!='interactive') & (RANGES[hyperparameter]['type']==2):\n",
    "                if not RANGES[hyperparameter]['condition']({'nlayers':condition}):\n",
    "                    continue\n",
    "                        \n",
    "            if set_type == 0:\n",
    "                occurrences_vector = [good_values.count(value)/(bad_values.count(value) + eps) for value in set_range]\n",
    "                index = np.argmax(occurrences_vector)\n",
    "                config[hyperparameter] = set_range[index]\n",
    "            if set_type == 1:\n",
    "                if set_sample == 0:\n",
    "                    def f(x):\n",
    "                        return sum_pdf(x, means = np.array(good_values,dtype=float), myclip_a = myclip_a,\n",
    "                                        myclip_b = myclip_b)/(sum_pdf(x, means =np.array(bad_values,dtype=float),\n",
    "                                                                      myclip_a = myclip_a, myclip_b = myclip_b))\n",
    "                    #config[hyperparameter] = scipy.optimize.minimize(f, (myclip_a + myclip_b)/2)['x'][0]\n",
    "                    sampled_points = sample_from_mixture(good_values, myclip_a, myclip_b, 50)\n",
    "                    index = np.argmax(f(np.array(sampled_points)))\n",
    "                    config[hyperparameter] = sampled_points[index]\n",
    "                if set_sample == 1:\n",
    "                    def f(x):\n",
    "                        return -sum_pdf(x, means = np.log(np.array(good_values,dtype=float)), myclip_a = np.log(myclip_a),\n",
    "                                        myclip_b = np.log(myclip_b))/(sum_pdf(x,\n",
    "                                means = np.log(np.array(bad_values,\n",
    "                                dtype=float)), myclip_a = np.log(myclip_a), myclip_b = np.log(myclip_b)) + eps)\n",
    "                    config[hyperparameter] = np.exp(scipy.optimize.minimize(f,\n",
    "                    (np.log(myclip_a) + np.log(myclip_b))/2)['x'][0])\n",
    "            if set_type == 2:\n",
    "                if set_sample == 0:\n",
    "                    def f(x):\n",
    "                        return -sum_pdf(x, means = np.array(good_values,dtype=float), myclip_a = myclip_a, \n",
    "                                        myclip_b = myclip_b)/(sum_pdf(x, means =np.array(bad_values,dtype=float), \n",
    "                                        myclip_a = myclip_a, myclip_b = myclip_b) + eps)\n",
    "                    config[hyperparameter]=round(scipy.optimize.minimize(f,(myclip_a + myclip_b)/2)['x'][0])\n",
    "            \n",
    "                if set_sample == 1:\n",
    "                    def f(x):\n",
    "                        return -sum_pdf(x, means = np.log(np.array(good_values,dtype=float)),myclip_a=np.log(myclip_a), \n",
    "                                        myclip_b = np.log(myclip_b))/(sum_pdf(x, means = np.log(np.array(bad_values,dtype=float)), \n",
    "                                        myclip_a = np.log(myclip_a), myclip_b = np.log(myclip_b)) + eps)\n",
    "                    config[hyperparameter] = round(np.exp(scipy.optimize.minimize(\n",
    "                        f, (np.log(myclip_a) + np.log(myclip_b))/2)['x'][0]))\n",
    "             \n",
    "            # this is necessary to tell whether the conditional hyperparameters should be considered or not\n",
    "            if hyperparameter == 'nlayers':\n",
    "                condition = config[hyperparameter]\n",
    "                \n",
    "        configs += [config]\n",
    "        \n",
    "        #report loss\n",
    "        loss = GET_CONFIG_PERFORMANCE(config, problem)\n",
    "        history += [loss]\n",
    "        \n",
    "    return history, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3f55ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "import scipy\n",
    "from utils import GET_CONFIG_PERFORMANCE, GET_RANGES, SampleType, ParamType # make sure to make use of ParamType and SampleType in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caca1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c132d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
